# -*- coding: utf-8 -*-
"""13_CS4501_BerRatingPredictions_D2_Leitrim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q-OE4fJlqtPtr8KeCNIDrGQzcRJkrToJ
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

import os

from sklearn.tree import plot_tree

from sklearn.preprocessing import LabelEncoder

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.impute import SimpleImputer

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder

if 'google.colab' in str(get_ipython()):
  from google.colab import drive
  drive.mount("/content/drive", force_remount=True)
  base_dir = "./drive/My Drive/Colab Notebooks/" # You may need to change this, depending on where your notebooks are on Google Drive
else:
  base_dir = "/content/drive/My Drive/Colab Notebooks/" # You may need to change this, depending on where your datasets folder is
dataset_dir = os.path.join(base_dir, "Datasets/dataset_ber4_leitrim.csv")  # Assuming the file is a CSV

print(os.listdir(base_dir + "Datasets/"))  # This will list all files in the Datasets directory

# Load the dataset
print(dataset_dir)
df = pd.read_csv(dataset_dir)

# Shuffle the dataset
df = df.sample(frac=1, random_state=2)
df.reset_index(drop=True, inplace=True)

df.shape

"""<h1>Preprocess the data</h1>"""

# Function to map BerRating values to EnergyRating categories
def map_ber_to_energy_rating(ber):
    if ber < 25:
        return "A1"
    elif 25 <= ber < 50:
        return "A2"
    elif 50 <= ber < 75:
        return "A3"
    elif 75 <= ber < 100:
        return "B1"
    elif 100 <= ber < 125:
        return "B2"
    elif 125 <= ber < 150:
        return "B3"
    elif 150 <= ber < 175:
        return "C1"
    elif 175 <= ber < 200:
        return "C2"
    elif 200 <= ber < 225:
        return "C3"
    elif 225 <= ber < 260:
        return "D1"
    elif 260 <= ber < 300:
        return "D2"
    elif 300 <= ber < 340:
        return "E1"
    elif 340 <= ber < 380:
        return "E2"
    elif 380 <= ber < 450:
        return "F"
    else:
        return "G"

"""**Load the Building Archetype Dataframe**"""

if 'google.colab' in str(get_ipython()):
  from google.colab import drive
  drive.mount("/content/drive", force_remount=True)
  base_dir = "./drive/My Drive/Colab Notebooks/" # You may need to change this, depending on where your notebooks are on Google Drive
else:
  base_dir = "/content/drive/My Drive/Colab Notebooks/" # You may need to change this, depending on where your datasets folder is
dataset_dir = os.path.join(base_dir, "Datasets/Ber_Archetype_Mean.csv")  # Assuming the file is a CSV

print(os.listdir(base_dir + "Datasets/"))  # This will list all files in the Datasets directory

# Load the dataset
print(dataset_dir)
archetype_df = pd.read_csv(dataset_dir)

archetype_df.head(20)

"""**Extract two properties for each of the specified energy rating categories (E1, E2, F, and G) from the dataset**"""

# Define the categories we're interested in
target_categories = ['E1', 'E2', 'F', 'G']

# Initialize an empty DataFrame to store selected properties
selected_properties = pd.DataFrame()

# Iterate over each category
for category in target_categories:
    # Filter properties from the copy_df dataset belonging to the current category
    properties_in_category = df[df['EnergyRating'] == category]

# If there are at least two properties in this category
    if len(properties_in_category) >= 2:
        # Select two properties randomly
        selected = properties_in_category.sample(2, random_state=42)
    elif len(properties_in_category) == 1:  # If there's exactly one property
        # Just take that one property
        selected = properties_in_category
    else:
        # If there are no properties left in this category due to NaN removal, report this
        print(f"No properties available in category {category} after NaN removal.")
        continue  # Skip adding to selected_properties

    # Add these properties to our selection
    selected_properties = pd.concat([selected_properties, selected])

# Reset index for the final DataFrame
selected_properties.reset_index(drop=True, inplace=True)

# Print the selected properties
print(selected_properties.iloc[:, :13])  # Display only the first 13 columns of the selected properties

# Create a copy of the selected_properties DataFrame
copy_selected_properties = selected_properties.copy()

print("Selected properties columns:", copy_selected_properties.columns)
print("Archetype dataframe columns:", archetype_df.columns)

# Iterate over each row in the copy_selected_properties DataFrame
for index, property_row in copy_selected_properties.iterrows():
    # Find the corresponding row in archetype_means_df
    matching_row = archetype_df[
        (archetype_df["DwellingType"] == property_row["DwellingTypeDescr"]) &
        (archetype_df["EnergyRating"] == "D2")
    ].iloc[0] if not archetype_df[
        (archetype_df["DwellingType"] == property_row["DwellingTypeDescr"]) &
        (archetype_df["EnergyRating"] == "D2")
    ].empty else None

    # If there is a matching row, update the UValues and other mean values if they are higher than the current ones
    if matching_row is not None:
        copy_selected_properties.at[index, "UValueWall"] = min(property_row["UValueWall"], matching_row["UValueWallMean"])
        copy_selected_properties.at[index, "UValueRoof"] = min(property_row["UValueRoof"], matching_row["UValueRoofMean"])
        copy_selected_properties.at[index, "UValueFloor"] = min(property_row["UValueFloor"], matching_row["UValueFloorMean"])
        copy_selected_properties.at[index, "UValueWindow"] = min(property_row["UValueWindow"], matching_row["UValueWindowMean"])
        copy_selected_properties.at[index, "UvalueDoor"] = min(property_row["UvalueDoor"], matching_row["UValueDoorMean"])
        #copy_selected_properties.at[index, "HSMainSystemEfficiency"] = max(property_row["HSMainSystemEfficiency"], matching_row["HSMainSystemEfficiencyMean"])

# Display the first 13 columns of the updated copy_selected_properties dataframe to verify changes
print(copy_selected_properties.iloc[:, :13])

# Define the path to the model file
model_path = '/content/drive/My Drive/Colab Notebooks/Models/streamlined_random_forest_regressor_model2.joblib'
processor_path = '/content/drive/My Drive/Colab Notebooks/Models/preprocessor2.joblib'

# Load the trained model
imported_model = joblib.load(model_path)
imported_processor = joblib.load(processor_path)

# Necessary features in DataFrame from Regression Model
required_features = ["PropertyID", "CountyName", "DwellingTypeDescr", "Year_of_Construction",
                     "TypeofRating", "TotalFloorArea", "UValueWall", "UValueRoof", "UValueFloor",
                     "UValueWindow", "UvalueDoor", "NoStoreys", "HSMainSystemEfficiency",
                     "HSSupplSystemEff", "WHMainSystemEff", "GroundFloorUValue", "SolarHotWaterHeating",
                     "InsulationType", "InsulationThickness", "DateOfAssessment"]

# Prepare the feature vector for prediction
X_new = copy_selected_properties[required_features]

# Load the trained model and preprocessor
imported_model = joblib.load('/content/drive/My Drive/Colab Notebooks/Models/streamlined_random_forest_regressor_model2.joblib')
imported_preprocessor = joblib.load('/content/drive/My Drive/Colab Notebooks/Models/preprocessor2.joblib')

# Apply the same transformations as during training
X_new_preprocessed = imported_preprocessor.transform(X_new)

# Predict new BerRatings using the imported model
new_ber_ratings = imported_model.predict(X_new_preprocessed)

# Convert predicted BerRatings to EnergyRating categories
new_energy_ratings = [map_ber_to_energy_rating(ber) for ber in new_ber_ratings]

# Add predictions back to the DataFrame
copy_selected_properties["Predicted BerRating"] = new_ber_ratings
copy_selected_properties["Predicted EnergyRating"] = new_energy_ratings

# Display the updated DataFrame
print(copy_selected_properties[["PropertyID", "EnergyRating", "BerRating",
                                "Predicted BerRating", "Predicted EnergyRating"]])

# Create a new figure and axis
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the original BerRatings
ax.scatter(copy_selected_properties["PropertyID"], copy_selected_properties["BerRating"], color='blue', label="Original BerRating")

# Plot the predicted BerRatings
ax.scatter(copy_selected_properties["PropertyID"], copy_selected_properties["Predicted BerRating"], color='red', label="Predicted BerRating")

# Set the plot title and labels
ax.set_title("Comparison of Original and Predicted BerRatings")
ax.set_xlabel("Property ID")
ax.set_ylabel("BerRating")

# Add a legend
ax.legend()

# Show the plot
plt.xticks(rotation=90) # Rotate PropertyID labels for better readability
plt.show()

# Define the path for the exported CSV file
export_file_path = '/content/drive/My Drive/Colab Notebooks/Datasets/Predicted_BerRatings_2.csv'

# Export the DataFrame to CSV
copy_selected_properties.to_csv(export_file_path, index=False)

print(f"Results exported successfully to {export_file_path}")